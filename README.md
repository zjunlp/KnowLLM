:speaking_head: \[ [‰∏≠Êñá](./README_ZH.md) | **English** \]

<p align="center">
    <br>
    <img src="https://github.com/zjunlp/KnowLM/blob/main/assets/KnowLM.png?raw=true" width="400" height="120"/>
    <br>
</p>

# Knowledgeable Large Language Model Framework

 

KnowLM  is a knowledgeable Large Language Model (LLM) framework, including data processing, model pre-training, fine-tuning, augmentation and utilization with knowledge.   Additionally, KnowLM provides a model zoo featuring readily accessible models like [ZhiXi](https://huggingface.co/zjunlp/knowlm-13b-zhixi) and [OneKE](http://oneke.openkg.cn/), tailored for immediate implementation.
- ‚ùóPlease note that this project is still undergoing optimization and developemnt, and the model weights will be regularly updated to support new features and models!
- ‚ùóIf you are interested in **information extraction/knowledge extraction**, please refer to the [DeepKE](https://github.com/zjunlp/DeepKE). KnowLM is a framework for knowledgeable Large Language Model!


**Features**

- A standard framework for LLM pre-training and fine-tuning.
- A model zoo including [ZhiXi](https://huggingface.co/zjunlp/knowlm-13b-zhixi), [KnowLM-IE](https://huggingface.co/zjunlp/knowlm-13b-ie), [OneKE](http://oneke.openkg.cn/), and [OceanGPT](https://www.zjukg.org/project/OceanGPT/), along with open-source data.
- A instruction processing module based on [EasyInstruct](https://github.com/zjunlp/EasyInstruct).
- A knowlege augmentation module based on RAG (under development).
- A hallucination detection module for based on [EasyDetect](https://github.com/zjunlp/EasyDetect).
- A knowlege editing module based on [EasyEdit](https://github.com/zjunlp/EasyEdit).
- Model inference and deployment.


All weights and datasets have been uploaded to HuggingFaceü§ó. Click [here](#1-1) to get started right away!

‚ùó**If you encounter any issues during the installation or use of KnowLM, please check [FAQ](https://github.com/zjunlp/KnowLM#6) or promptly submit an [issue](https://github.com/zjunlp/KnowLM/issues), and we will assist you with resolving the problem!**

| Category | Base   | Name                     | Version | Download Link                                                     | Note     |
| -------- | ------ | ------------------------- | ---- | ------------------------------------------------------------ | -------- |
| Base Model | LlaMA1 | KnowLM-13B-Base           | V1.0 | [HuggingFace](https://huggingface.co/zjunlp/knowlm-13b-base-v1.0) <br/> [WiseModel](https://wisemodel.cn/models/zjunlp/knowlm-13b-base-v1.0) <br/> [ModelScope](https://modelscope.cn/models/ZJUNLP/knowlm-13b-base-v1.0/) | Base Model |
| Dialogue Model | LlaMA1 | KnowLM-13B-ZhiXi          | V1.0 | [HuggingFace](https://huggingface.co/zjunlp/knowlm-13b-zhixi)  <br/> [WiseModel](https://wisemodel.cn/models/zjunlp/knowlm-13b-zhixi) <br/> [ModelScope](https://modelscope.cn/models/ZJUNLP/knowlm-13b-zhixi)| Information Extraction Model |
| Dialogue Model | LlaMA1 | KnowLM-13B-IE             | V1.0 | [HuggingFace](https://huggingface.co/zjunlp/knowlm-13b-ie) <br/> [WiseModel](https://wisemodel.cn/models/zjunlp/knowlm-13b-ie) <br/> [ModelScope](https://modelscope.cn/models/ZJUNLP/knowlm-13b-ie) | Information Extraction Model |
| Dialogue Model | LlaMA2 | OceanGPT | V1.0 | [HuggingFace](https://huggingface.co/zjunlp/OceanGPT-7b) <br/> [WiseModel](https://wisemodel.cn/models/zjunlp/OceanGPT-7b)| Ocean Model |
| Dialogue Model | LlaMA2 | OneKE                | V1.0 |  [HuggingFace](https://huggingface.co/openkg/OneKE)  <br/> [WiseModel](https://modelscope.cn/models/ZJUNLP/OneKE) <br/> [ModelScope](https://wisemodel.cn/models/zjunlp/OneKE)                                                    | Information Extraction Model |

| Instruction Dataset Name                        | Number    | Download Link                                                    |  Note                           |
| ------------------------------- | ------- | ------------------------------------------------------------ | ------------------------------ |
| KnowLM-CR (CoT&Reasoning, Chinese and English) | 202,333 | [Google Drive](https://drive.google.com/drive/folders/1iJgksjOStk0m9GM0RP9jB6KdNWfJ62Xe?usp=sharing) <br/> [HuggingFace](https://huggingface.co/datasets/zjunlp/KnowLM-CR)|                            |
| KnowLM-Tool (Tool LearningÔºåEnglish)     | 38,241  | [Google Drive](https://drive.google.com/file/d/1PyzXXv_pr2T-FysnCumWTDzFNCvtLDv2/view?usp=sharing) <br/> [HuggingFace](https://huggingface.co/datasets/zjunlp/KnowLM-Tool) |                              |
| OceanBench (BenchmarkÔºåEnglish)     | 11,000  |  [HuggingFace](https://huggingface.co/datasets/zjunlp/OceanBench) |  |                          |
| InstructIE (Information Extraction, Chinese and English) | 364, 076 | [HuggingFace](https://huggingface.co/datasets/zjunlp/InstructIE) <br/> [WiseModel](https://www.wisemodel.cn/datasets/zjunlp/InstructIE) <br/> [ModelScope](https://www.modelscope.cn/datasets/ZJUNLP/InstructIE) |   Due to using distant supervision, there exists noise.                            |
| IEPile (Information Extraction, Chinese and English) | 2,000,000 + | [HuggingFace](https://huggingface.co/datasets/zjunlp/iepile) <br/> [WiseModel](https://www.wisemodel.cn/datasets/zjunlp/IEPile) <br/> [ModelScope](https://www.modelscope.cn/datasets/ZJUNLP/IEPile) | It is constructed based on 33 exsiting IE datasets.                             |

**Data description**: 1. Other data sources for information extraction come from `CoNLL`, `ACE`, `casis`, `DuEE`, `People Daily`, `DuIE`, etc. 2. The `KnowLM-Tool` dataset comes from the paper "[Making Language Models Better Tool Learners with Execution Feedback](https://arxiv.org/abs/2305.13068)" and the [gitHub](https://github.com/zjunlp/trice) can be found here. 3. The `InstructIE` dataset comes from the paper "[InstructIE: A Chinese Instruction-based Information Extraction Dataset](https://arxiv.org/abs/2305.11527)" and the [gitHub](https://github.com/zjunlp/DeepKE/tree/main/example/llm/InstructKGC) can be found here.

## üì¨ NEWS
- \[**April 2024**\] We release a new bilingual (Chinese and English) schema-based information extraction model called [OneKE](https://huggingface.co/zjunlp/OneKE)  based on Chinese-Alpaca-2-13B.
- \[**March 2024**\] We release a new paper: "[KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents](https://arxiv.org/abs/2403.03101)".
- \[**February 2024**\] We release a large-scale (0.32B tokens) high-quality bilingual (Chinese and English) Information Extraction (IE) instruction dataset named [IEPile](https://huggingface.co/datasets/zjunlp/iepie), along with two models trained with `IEPile`, [baichuan2-13b-iepile-lora](https://huggingface.co/zjunlp/baichuan2-13b-iepile-lora) and [llama2-13b-iepile-lora](https://huggingface.co/zjunlp/llama2-13b-iepile-lora).
- \[**February 2024**\] We release a new paper: "[EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049)" with an HF demo [EasyInstruct](https://huggingface.co/spaces/zjunlp/EasyInstruct).
- \[**January 2024**\] We release a new paper:"[A Comprehensive Study of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2401.01286)" with a new benchmark [KnowEdit](https://huggingface.co/datasets/zjunlp/KnowEdit).
- \[**August 2023**\] The full parameters have been released (omitting the parameter consolidation process).
- \[**July 2023**\] The instruction dataset has been released.
- \[**July 2023**\] Support instruction fine-tuning and vllm for `LLaMA-2`
- \[**June 2023**\] The project name has been changed from `CaMA` to `KnowLM`.
- \[**June 2023**\] Release the first version of pre-trained weights and the LoRA weights.

## üìç Technologies in KnowLM
<p align="center">
    <br>
    <img src="https://github.com/zjunlp/KnowLM/blob/main/assets/KnowLM-overview.png?raw=true" width="920" height="400"/>
    <br>
</p>

This is an overview of the `KnowLM`, which mainly consists of three technical features:

**Knowledge Prompting**: It generates knowledge prompts based on structured data such as knowledge graphs and utilizes knowledge augmentation constraints to address *knowledge extraction and reasoning* issues.

**Knowledge Editing**: It aligns outdated, incorrect, and biased knowledge within large models using knowledge editing techniques to tackle *knowledge fallacy* problems (**[English Tutorial](./pdf/Knowledge_Editing.pdf)**).

**Knowledge Interaction**: It enables dynamic knowledge interaction and feedback to achieve tool-based learning and multi-agent collaboration, resolving the problem of *embodiment cognition* in LLMs (**[English Tutorial](./pdf/Knowledge_Interaction.pdf)**).

The modules related to these three technologies are [EasyInstruct](https://github.com/zjunlp/easyinstruct), [EasyDetect](https://github.com/zjunlp/EasyDetect), [EasyEdit](https://github.com/zjunlp/EasyEdit). We provide use cases for those modules based on the `KnowLM`framework.

## üóÇÔ∏è Contents

- [üö¥Quick Start](#1-quick-start)
  - [üõ†Ô∏èEnvironment Configuration](#%EF%B8%8F11-environment-configuration)
  - [üíªModel Usage Guide](#12-model-usage-guide)
  - [üéØInformation Extraction prompt](#13-information-extraction-prompt)
  - [üêêLlama.cpp](#14-llamacpp)
  - [üìåInstruction Processing](#15-instruction-processing)
  - [üñäÔ∏èModel Editing](#%EF%B8%8F16-model-editing)

- [üå∞Cases](#2-cases)
  - [üå∞Pretraining Cases](#21-pretraining-cases)
  - [üå∞Information Extraction Cases](#22-information-extraction-cases)
  - [üå∞General Ability Cases](#23-general-ablities-cases)
  - [üå∞Model Editing Cases](#24-model-editing-cases)
- [ü•äTraining Details](#3-training-details)
  - [üßæPertraining data and Pretraining scripts](#31-dataset-construction-pretraining)
  - [üßæInstruction data and Instruction-tuning scripts](#32-training-process-pretraining)
- [üî¥Limitations](#4-limitations)
- [üïêTODO List](#5-todo-list)
- [‚ùìFAQ](#6-faq)
- [üëãAcknowledgments/Contributors/Citations](#7-others)

### All Thanks To Our Contributors :
<a href="https://github.com/zjunlp/KnowLM/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=zjunlp/KnowLM" />
</a>


<h2 id="1">üö¥1. Quick Start</h2>

<h3 id="1-1">üõ†Ô∏è1.1 Environment Configuration</h3>

*KnowLM* supports both **manual** and **docker image** environment configuration, you can choose the appropriate way to build.
#### üîßManual Environment Configuration
```shell
git clone https://github.com/zjunlp/KnowLM.git
cd KnowLM
conda create -n knowlm python=3.9 -y
conda activate knowlm
pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
pip install -r requirements.txt
```
#### üê≥Building With Docker Images
```shell
docker pull zjunlp/knowlm:v.1
docker run -it zjunlp/knowlm:v.1 /bin/bash
```
<h3 id="1-2">üíª1.2 Model Usage Guide</h3>

**1. Reproduce the results in Section 2**

> The cases in **[Section 2](#2-cases)** were all run on V100. If running on other devices, the results may vary. Please run multiple times or change the decoding parameters. **We derived `knowlm-13b-zhixi` and `knowlm-13b-ie` through training using LoRA, building upon the foundation of `knowlm-13b-base`. These models, `knowlm-13b-zhixi` and `knowlm-13b-ie`, are the result of merging the trained LoRA weights with the existing `knowlm-13b-base` model parameters.**

1. If you want to reproduce the results in `section 2.1`(**[pretraining cases](#21-pretraining-cases)**), please run the following command:

   ```shell
   python examples/generate_finetune.py --base_model zjunlp/knowlm-13b-base-v1.0
   ```

   The result in section `2.1` can be obtained.

2. If you want to reproduce the results in `section 2.2`(**[information extraction cases](#22-information-extraction-cases)**), please run the following command:

   ```shell
   python examples/generate_lora.py --base_model zjunlp/knowlm-13b-zhixi --run_ie_cases
   ```

   The result in section `2.2` can be obtained.

3. If you want to reproduce the results in `section 2.3`(**[general abilities cases](#23-general-abilities-cases)**), please run the following command:

   ```shell
   python examples/generate_lora.py --base_model zjunlp/knowlm-13b-zhixi --run_general_cases
   ```

   The result in section `2.3` can be obtained.


**2. Usage of Pretraining Model**

We offer two methods: the first one is **command-line interaction**, and the second one is **web-based interaction**, which provides greater flexibility.

1. Use the following command to enter **command-line interaction**:

   ```shell
   python examples/generate_finetune.py --base_model zjunlp/knowlm-13b-base-v1.0 --interactive
   ```

   The disadvantage is the inability to dynamically change decoding parameters.

   If a single GPU is unable to load the model, you can utilize the following command to enable the model to be loaded across different GPU:

   ```shell
   CUDA_VISIBLE_DEVICES=0,1,2 python examples/generate_finetune.py  --base_model zjunlp/knowlm-13b-base-v1.0 --interactive --multi_gpu     # --allocate [10,10,10]
   ```

   > The `--allocate` above specifies the amount of memory used by each GPU, measured in `GB`.

3. Use the following command to enter **web-based interaction**:

   ```shell
   python examples/generate_finetune_web.py --base_model zjunlp/knowlm-13b-base-v1.0
   ```

   If a single GPU is unable to load the model, you can utilize the following command to enable the model to be loaded across different GPU:

   ```shell
   CUDA_VISIBLE_DEVICES=0,1,2 python examples/generate_finetune_web.py --base_model zjunlp/knowlm-13b-base-v1.0 --multi_gpu     # --allocate [10,10,10]
   ```
   
   Here is a screenshot of the web-based interaction:
   <p align="center" width="100%">
   <a href="" target="_blank"><img src="./assets/finetune_web.jpg" alt="finetune-web" style="width: 100%; min-width: 100px; display: block; margin: auto;"></a>
   </p>

**3. Usage of Instruction tuning Model**

Here, we provide a web-based interaction method. Use the following command to access the web:

```shell
python examples/generate_lora_web.py --base_model zjunlp/knowlm-13b-zhixi
```

If a single GPU is unable to load the model, you can utilize the following command to enable the model to be loaded across different GPU:

```shell
CUDA_VISIBLE_DEVICES=0,1,2 python examples/generate_lora_web.py --base_model zjunlp/knowlm-13b-zhixi --multi_gpu     # --allocate [10,10,10]
```

Here is a screenshot of the web-based interaction:
<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/lora_web.png" alt="finetune-web" style="width: 100%; min-width: 100px; display: block; margin: auto;"></a>
</p>

The `instruction` is a required parameter, while `input` is an optional parameter. For general tasks (such as the examples provided in section `1.3`), you can directly enter the input in the `instruction` field. For information extraction tasks (as shown in the example in section `1.2`), please enter the instruction in the `instruction` field and the sentence to be extracted in the `input` field. We provide an information extraction prompt in section `2.5`. 

If you want to perform batch testing, please modify the `examples/generate_lora.py` file and update the examples and hyperparameters in the variable `cases`.

According to different task requirements, we have the following suggestions for adjusting decoding strategies and their associated hyperparameters:

1. If you want more diverse and creative outputs, consider using top-k or top-p (nucleus) sampling with a relatively higher `top_k` or `top_p`, and possibly a higher `temperature`.
2. If you want more focused and high-quality outputs (e.g., information extraction), consider using beam search with a moderate `num_beam`, or top-k or top-p sampling with a lower `top_k` or `top_p`, and a lower `temperature`.
3. Remember to experiment and fine-tune. Depending on your use case, it may be beneficial to iterate and experiment with different strategies and hyperparameters to find the optimal combination.

**4. vLLM API server**

We integrate [vLLM](https://github.com/vllm-project/vllm) for accelerating LLM inference and providing efficient API service. Use the following command to launch vLLM API server at `http://localhost:8090`.

```shell
max_num_batched_tokens=8000

CUDA_VISIBLE_DEVICES=1,2 python inference/launch_vllm.py \
    --port 8090 \
    --model data/zhixi-13B \
    --use-np-weights \
    --max-num-batched-tokens $max_num_batched_tokens \
    --dtype half \
    --tensor-parallel-size 2
```

Query the service using POST request:

```shell
curl -X POST "http://127.0.0.1:8090/generate" \
  -H 'Content-Type: application/json' \
  -d '{"instruction": "‰Ω†Â•Ω", "input": "", "parameters": {"top_p": 0.7, "max_tokens": 256}}'
```

You could get the following response:

```shell
{
  "generated_text":"‰Ω†Â•ΩÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†„ÄÇÊàëÊòØ‰∏Ä‰∏™‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÔºåÂèØ‰ª•Â∏ÆÂä©‰Ω†Ëß£ÂÜ≥ÈóÆÈ¢òÂíåÊèê‰æõ‰ø°ÊÅØ„ÄÇÊúâ‰ªÄ‰πàÊàëÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂêóÔºü</s>",
  "num_output_tokens_cf":65,
  "error":null
}
```

<h3 id="1-3">üéØ1.3 Information Extraction Prompt</h3>

For information extraction tasks such as named entity recognition (NER), event extraction (EE), and relation extraction (RE), we provide some prompts for ease of use. You can refer to this [link](./examples/ie_prompt.py) for examples. Of course, you can also try using your own prompts.

Here is a [case](https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md) where `knowlm-13b-zhixi` is used to accomplish the instruction-based knowledge graph construction task in CCKS2023.

<h3 id="1-4">üêê1.4 LlaMA.cpp</h3>

If you find yourself lacking sufficient GPU computing resources, you have the option to carry out quantization using [llama.cpp](https://github.com/ggerganov/llama.cpp). This is possible because llama.cpp shares the same architecture as KnowLM. Once you have set up your environment, you can download our model to a designated path using the following command:
```bash
python tools/download.py --specify --download_path ./your/path --repo_name zjunlp/knowlm-13b-zhixi
```
Next, just substitute the model path at this [location](https://github.com/ggerganov/llama.cpp#prepare-data--run) with the downloaded one. When executing it in practice, please remember to adjust the model path within this [script](https://github.com/ggerganov/llama.cpp/blob/master/examples/alpaca.sh) accordingly.

<h3 id="1-5">üìå1.5 Instruction Processing</h3>

Instruction tuning has emerged as a crucial technique to enhance the capabilities of LLMs, which bridges the gap between the next-word prediction objective of LLMs and human preference.
To construct a high-quality instruction dataset, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality.

In instruction processing, we utilized EasyInstruct as our processing framework (detailed can be found at https://github.com/zjunlp/EasyInstruct). EasyInstruct modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. The code below shows a running example of instruction generation and selection in EasyInstruct:

```python
from easyinstruct import SelfInstructGenerator, GPTScoreSelector
from easyinstruct.utils.api import set_openai_key

# Step1: Set your own API-KEY
set_openai_key("YOUR-KEY")

# Step2: Declare a generator class
generator = SelfInstructGenerator(num_instructions_to_generate=100)

# Step3: Generate self-instruct data
generator.generate()

# Step4: Declare a selector class
selector = GPTScoreSelector()

# Step5: Process the generated instructions
selector.process()
```

<h3 id="1-6">üñäÔ∏è1.6 Model Editing</h3>

Although large language models perform exceptionally well in many tasks, they can still provide incorrect answers. Moreover, as time passes, knowledge that was once accurate may become outdated. This necessitates that we adjust the model's responses to meet our expectations through model editing.

In model editing, we utilized EasyEdit as our editing tool (details can be found at https://github.com/zjunlp/EasyEdit). EasyEdit is a highly integrated model editing tool. All you need to do is define your editor in just three lines of code, similar to how you would in hugging face.

```python
from easyeditor import MENDHyperParams
hparams = MENDHyperParams.from_hparams('./hparams/MEND/gpt2-xl')
editor = BaseEditor.from_hparams(hparams)
```
The code above demonstrates the editor definition for editing the gpt2-xl model using the MEND method. The next step is to prepare the editing data and the test data.

```python
metrics, edited_model, _ = editor.edit(
    prompts=prompts,
    ground_truth=ground_truth,
    target_new=target_new,
    locality_inputs=locality_inputs,
    keep_original_weight=True
)
```
With the provided code, you can complete the editing of the model. The edited model is stored in "edit_model," and the corresponding evaluation metrics are saved in "metrics."

<h2 id="2">üå∞2. Cases</h2>

<h3 id="2-1">üå∞2.1 Pretraining Cases</h3>

Our pre-trained model has demonstrated certain abilities in instruction following, coding, reasoning, as well as some translation capabilities, without any fine-tuning using instructions. Additionally, it has acquired new knowledge. Below are some of our sample cases. If you wish to reproduce our examples and view detailed decoding configuration, please first [set up the environment](#1-1), then follow the steps outlined [here](#1-2).

> In the following cases, text in **bold** represents the prompt, while non-bold text represents the model's output.
>
> Due to the maximum inference length set to 512, our cases fall into three situations:
>
> 1. Completed output. The model generates the termination token EOS and completes the output. We mark this with :white_check_mark:.
> 2. Incomplete output. The output is cut off due to the maximum inference length. We mark this with :eight_spoked_asterisk:.
> 3. Repeated output. We remove repeated content manually and mark it with :arrow_left:.

<details>
  <summary><b>Translation</b></summary>


  - <b>‰Ω†ÂæàÊìÖÈïøÂ∞Ü‰∏≠ÊñáÁøªËØëÊàêËã±Êñá„ÄÇÂ∞Ü‰∏ãÈù¢ÁöÑÂè•Â≠êÁøªËØëÊàêËã±ÊñáÔºöÊàë‰ª¨‰ªäÂ§©ÂáÜÂ§áÂéªË•øÂÆâÁúãÂÖµÈ©¨‰øë„ÄÇÁ≠îÊ°àÔºö</b>We are going to see the Terracotta Warriors in Xi'an today.:white_check_mark:

- **You are good at translating English into Chinese. Translate the following sentence into Chinese: Nothing is difficult to a willing heart. Answer:** ‰ªÄ‰πàÈÉΩ‰∏çÈöæ,Âè™Ë¶ÅÂøÉÁîòÊÉÖÊÑø„ÄÇ:white_check_mark:
  </details>
  

<details>
  <summary><b>Knowledge</b></summary>


- **‰Ω†ÈòÖËØªËøáÊùéÁôΩÁöÑÊâÄÊúâËØóÊ≠å„ÄÇÊùéÁôΩÁöÑ„ÄäÂ∞ÜËøõÈÖí„ÄãÁöÑÂéüÊñáÊòØ**:Âêõ‰∏çËßÅÈªÑÊ≤≥‰πãÊ∞¥Â§©‰∏äÊù•,Â•îÊµÅÂà∞Êµ∑‰∏çÂ§çÂõû„ÄÇ Âêõ‰∏çËßÅÈ´òÂ†ÇÊòéÈïúÊÇ≤ÁôΩÂèë,ÊúùÂ¶ÇÈùí‰∏ùÊöÆÊàêÈõ™„ÄÇ ‰∫∫ÁîüÂæóÊÑèÈ°ªÂ∞ΩÊ¨¢,Ëé´‰ΩøÈáëÊ®ΩÁ©∫ÂØπÊúà„ÄÇ Â§©ÁîüÊàëÊùêÂøÖÊúâÁî®,ÂçÉÈáëÊï£Â∞ΩËøòÂ§çÊù•„ÄÇ ÁÉπÁæäÂÆ∞Áâõ‰∏î‰∏∫‰πê,‰ºöÈ°ª‰∏ÄÈ•Æ‰∏âÁôæÊùØ„ÄÇ Â≤ëÂ§´Â≠ê,‰∏π‰∏òÁîü,Â∞ÜËøõÈÖí,ÂêõËé´ÂÅú„ÄÇ ‰∏éÂêõÊ≠å‰∏ÄÊõ≤,ËØ∑Âêõ‰∏∫Êàë‰æßËÄ≥Âê¨„ÄÇ ÈíüÈºìÈ¶îÁéâ‰ΩïË∂≥Ë¥µ,‰ΩÜÊÑøÈïøÈÜâ‰∏çÂ§çÈÜí„ÄÇ Âè§Êù•Âú£Ë¥§ÁöÜÂØÇÂØû,ÊÉüÊúâÈ•ÆËÄÖÁïôÂÖ∂Âêç„ÄÇ ÈôàÁéãÊòîÊó∂ÂÆ¥Âπ≥‰πê,ÊñóÈÖíÂçÅÂçÉÊÅ£Ê¨¢Ë∞ë„ÄÇ ‰∏ª‰∫∫‰Ωï‰∏∫Ë®ÄÂ∞ëÈí±,ÂæÑÈ°ªÊ≤ΩÂèñÂØπÂêõÈÖå„ÄÇ ‰∫îËä±È©¨,ÂçÉÈáëË£ò,ÂëºÂÑøÂ∞ÜÂá∫Êç¢ÁæéÈÖí,‰∏éÂ∞îÂêåÈîÄ‰∏áÂè§ÊÑÅ„ÄÇ Âîê‰ª£Â§ßËØó‰∫∫ÊùéÁôΩ(701-762),Â≠óÂ§™ÁôΩ,Âè∑ÈùíËé≤Â±ÖÂ£´„ÄÇ Á•ñÁ±çÈôáË•øÊàêÁ∫™(‰ªäÁîòËÇÉÂ§©Ê∞¥),ÈöãÊú´ÂÖ∂ÂÖà‰∫∫ÊµÅÂØìË•øÂüüÁ¢éÂè∂(ÂîêÊó∂Â±ûÂÆâË•øÈÉΩÊä§Â∫ú,Âú®‰ªäÂêâÂ∞îÂêâÊñØÊñØÂù¶ÂåóÈÉ®ÊâòÂÖãÈ©¨ÂÖãÈôÑËøë),ÊùéÁôΩÂç≥Áîü‰∫éÁ¢éÂè∂„ÄÇ ÂπºÊó∂ÈöèÁà∂ËøÅÂ±ÖÁªµÂ∑ûÊòåÈöÜ(‰ªäÂõõÂ∑ùÊ±üÊ≤π)ÈùíËé≤‰π°:eight_spoked_asterisk:
- **Â∫äÂâçÊòéÊúàÂÖâÔºåÁñëÊòØÂú∞‰∏äÈúú„ÄÇ** ‰∏æÂ§¥ÊúõÊòéÊúàÔºå‰ΩéÂ§¥ÊÄùÊïÖ‰π°„ÄÇ ËøôÈ¶ñËØóÂá∫Ëá™Âîê‰ª£ËØó‰∫∫ÊùéÁôΩÁöÑ„ÄäÈùôÂ§úÊÄù„Äã„ÄÇ ‰ΩúÂìÅÂéüÊñá Â∫äÂâçÊòéÊúàÂÖâÔºåÁñëÊòØÂú∞‰∏äÈúú„ÄÇ ‰∏æÂ§¥ÊúõÊòéÊúàÔºå‰ΩéÂ§¥ÊÄùÊïÖ‰π°„ÄÇ  [1] ‰ΩúËÄÖÁÆÄ‰ªã ÊùéÁôΩÔºà701ÔΩû762ÔºâÂ≠óÂ§™ÁôΩÔºåÂè∑ÈùíËé≤Â±ÖÂ£´„ÄÇ Á•ñÁ±çÈôáË•øÊàêÁ∫™Ôºà‰ªäÁîòËÇÉÂ§©Ê∞¥ÈôÑËøëÔºâÔºåÂÖà‰∏ñ‰∫éÈöãÊú´ÊµÅÂæôË•øÂüüÔºåÊùéÁôΩÂç≥Áîü‰∫é‰∏≠‰∫öÁ¢éÂè∂Ôºà‰ªäÂ∑¥Â∞îÂñÄ‰ªÄÊπñÂçóÈù¢ÁöÑÊ•öÊ≤≥ÊµÅÂüüÔºåÂîêÊó∂Â±ûÂÆâË•øÈÉΩÊä§Â∫úÁÆ°ËæñÔºâ„ÄÇ ÂπºÊó∂ÈöèÁà∂ËøÅÂ±ÖÁªµÂ∑ûÊòåÈöÜÔºà‰ªäÂõõÂ∑ùÊ±üÊ≤πÔºâÈùíËé≤‰π°„ÄÇ ‰ªñ‰∏ÄÁîüÁªùÂ§ßÈÉ®ÂàÜÂú®Êº´Ê∏∏‰∏≠Â∫¶Ëøá„ÄÇ ÂÖ¨ÂÖÉ742Âπ¥ÔºàÂ§©ÂÆùÂÖÉÂπ¥ÔºâÔºåÂõ†ÈÅìÂ£´Âê¥Á≠†ÁöÑÊé®ËçêÔºåË¢´Âè¨Ëá≥ÈïøÂÆâÔºå‰æõÂ•âÁø∞Êûó„ÄÇ ÊñáÁ´†È£éÈááÔºåÂêçÂä®‰∏ÄÊó∂ÔºåÈ¢á‰∏∫ÂîêÁéÑÂÆóÊâÄËµèËØÜ„ÄÇ ÂêéÂõ†‰∏çËÉΩËßÅÂÆπ‰∫éÊùÉË¥µÔºåÂú®‰∫¨‰ªÖ‰∏âÂπ¥ÔºåÂ∞±ÂºÉÂÆòËÄåÂéªÔºå‰ªçÁÑ∂ÁªßÁª≠‰ªñÈÇ£È£òËç°ÂõõÊñπÁöÑÊµÅÊµ™ÁîüÊ¥ª„ÄÇ ÂÖ¨ÂÖÉ756Âπ¥ÔºåÂç≥ÂÆâÂè≤‰πã‰π±ÂèëÁîüÁöÑÁ¨¨‰∫åÂπ¥Ôºå‰ªñÊÑüÊÑ§Êó∂Ëâ∞ÔºåÊõæÂèÇÂä†‰∫ÜÊ∞∏ÁéãÊùéÁíòÁöÑÂπïÂ∫ú„ÄÇ ‰∏çÂπ∏ÔºåÊ∞∏Áéã‰∏éËÇÉÂÆóÂèëÁîü‰∫Ü‰∫âÂ§∫Â∏ù‰ΩçÁöÑÊñó‰∫âÔºåÂ§±Ë¥•‰πã:eight_spoked_asterisk:
- **Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊòØ**ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÁöÑ‰∏Ä‰∏™ÂàÜÊîØÔºåÂÆÉÁ†îÁ©∂Â¶Ç‰Ωï‰ªéËá™ÁÑ∂ËØ≠Ë®Ä‰∏≠ÊäΩÂèñÂá∫ÊúâÊÑè‰πâÁöÑ‰ø°ÊÅØ„ÄÇ Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂåÖÊã¨ËØçÊ≥ïÂàÜÊûê„ÄÅÂè•Ê≥ïÂàÜÊûê„ÄÅËØ≠‰πâÂàÜÊûê„ÄÅËØ≠Èü≥ËØÜÂà´ÂíåÊú∫Âô®ÁøªËØëÁ≠â„ÄÇ Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑÁõÆÊ†áÊòØ‰ΩøËÆ°ÁÆóÊú∫ËÉΩÂ§üÂÉè‰∫∫Á±ª‰∏ÄÊ†∑ÁêÜËß£Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇ:white_check_mark:
</details>


<details>
  <summary><b>Instruction Following</b></summary>


- <b>‰Ω†ÈùûÂ∏∏‰∫ÜËß£‰∏Ä‰∫õÂÅ•Â∫∑ÁîüÊ¥ªÁöÑ‰π†ÊÉØÔºåËØ∑Âàó‰∏æÂá†‰∏™ÂÅ•Â∫∑ÁîüÊ¥ªÁöÑÂª∫ËÆÆÔºö</b>1.ÊØèÂ§©ÂùöÊåÅÈîªÁÇº30ÂàÜÈíü‰ª•‰∏ä„ÄÇ  2.‰∏çÂê∏ÁÉüÔºå‰∏çÈÖóÈÖí„ÄÇ  3.Â∞ëÂêÉÈ´òËÑÇËÇ™È£üÁâ©„ÄÇ  4.Â§öÂêÉËî¨ËèúÂíåÊ∞¥Êûú„ÄÇ  5.‰øùËØÅÂÖÖË∂≥ÁöÑÁù°Áú†„ÄÇ  6.‰øùÊåÅËâØÂ•ΩÁöÑÂøÉÊÉÖ„ÄÇ  7.ÂÆöÊúü‰ΩìÊ£Ä„ÄÇ  8.ÂÖªÊàêËâØÂ•ΩÁöÑÂç´Áîü‰π†ÊÉØ„ÄÇ:white_check_mark:

- **ÊàëÁà±‰Ω†ÁöÑËã±ÊñáÊòØ‰ªÄ‰πàÔºü** ÊàëÁà±‰Ω†ÁöÑËã±ÊñáÊòØ"i love you"„ÄÇ:arrow_left:

- **‰Ω†ÂØπ‰∏≠ÂõΩÁöÑÂ§ßÂ≠¶‰∫ÜËß£ÁöÑÈùûÂ∏∏Â§ö„ÄÇËØ∑ËØÑ‰ª∑‰∏Ä‰∏ãÊµôÊ±üÂ§ßÂ≠¶ÊòØ‰∏ÄÊâÄÊÄéÊ†∑ÁöÑÂ≠¶Ê†°„ÄÇ** Á≠î:ÊµôÊ±üÂ§ßÂ≠¶ÊòØ‰∏ÄÊâÄÂéÜÂè≤ÊÇ†‰πÖ„ÄÅÂ≠¶ÁßëÈΩêÂÖ®„ÄÅÂ≠¶ÊúØÂÆûÂäõÈõÑÂéö„ÄÅÂ∏àËµÑÂäõÈáèÂº∫Â§ß„ÄÅÂäûÂ≠¶ÁâπËâ≤È≤úÊòé,Âú®ÂõΩÂÜÖÂ§ñÂÖ∑ÊúâÈáçË¶ÅÂΩ±ÂìçÁöÑÁªºÂêàÊÄßÁ†îÁ©∂ÂûãÂ§ßÂ≠¶„ÄÇ ÊµôÊ±üÂ§ßÂ≠¶ÊòØÈ¶ñÊâπËøõÂÖ•ÂõΩÂÆ∂"211Â∑•Á®ã"Âíå"985Â∑•Á®ã"Âª∫ËÆæÁöÑÈ´òÊ†°‰πã‰∏Ä,‰πüÊòØÂõΩÂÆ∂"111ËÆ°Âàí"Âíå"Áè†Â≥∞ËÆ°Âàí"ÈáçÁÇπÂª∫ËÆæÁöÑÂêçÁâåÂ§ßÂ≠¶„ÄÇ ÊµôÊ±üÂ§ßÂ≠¶ÁöÑÂâçË∫´ÊòØÂàõÂª∫‰∫é1897Âπ¥ÁöÑÊ±ÇÊòØ‰π¶Èô¢,ÊòØ‰∏≠ÂõΩ‰∫∫ÊúÄÊó©Ëá™Â∑±ÂàõÂäûÁöÑÊñ∞ÂºèÈ´òÁ≠âÂ≠¶Â∫ú‰πã‰∏Ä„ÄÇ 1928Âπ¥,Ê±ÇÊòØ‰π¶Èô¢ÊîπÂà∂‰∏∫ÂõΩÁ´ãÊµôÊ±üÂ§ßÂ≠¶„ÄÇ 1952Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Êàê‰∏∫Êñ∞‰∏≠ÂõΩÂàõÂäûÁöÑÁ¨¨‰∏ÄÊâπÂÖ®ÂõΩÈáçÁÇπÂ§ßÂ≠¶„ÄÇ 1998Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶‰∏éÊù≠Â∑ûÂ§ßÂ≠¶„ÄÅÊµôÊ±üÂÜú‰∏öÂ§ßÂ≠¶„ÄÅÊµôÊ±üÂåªÁßëÂ§ßÂ≠¶„ÄÅÊµôÊ±üÂ∏àËåÉÂ§ßÂ≠¶„ÄÅÊµôÊ±üÂ∏àËåÉÂ§ßÂ≠¶ÂåªÂ≠¶Èô¢ÂêàÂπ∂ÁªÑÂª∫Êñ∞ÁöÑÊµôÊ±üÂ§ßÂ≠¶„ÄÇ ÊµôÊ±üÂ§ßÂ≠¶Áé∞Â∑≤ÂèëÂ±ïÊàê‰∏∫‰∏ÄÊâÄÂåÖÊã¨Êñá„ÄÅÊ≥ï„ÄÅÁêÜ„ÄÅÂ∑•„ÄÅÂÜú„ÄÅÂåª„ÄÅÁÆ°„ÄÅËâ∫Á≠âÂ≠¶ÁßëÈó®Á±ªÈΩêÂÖ®ÁöÑÁªºÂêàÊÄßÁ†îÁ©∂ÂûãÂ§ßÂ≠¶,ÊòØÂõΩÂÆ∂"985Â∑•Á®ã"Âíå"211Â∑•Á®ã"ÈáçÁÇπÂª∫ËÆæÁöÑÂ§ßÂ≠¶‰πã‰∏Ä,‰πüÊòØÂõΩÂÆ∂"111ËÆ°Âàí"Âíå"Áè†Â≥∞ËÆ°Âàí"ÈáçÁÇπÂª∫ËÆæÁöÑÂêçÁâåÂ§ßÂ≠¶„ÄÇ:white_check_mark:

- **‰Ω†ÈùûÂ∏∏‰∫ÜËß£‰∏≠ÂõΩÁöÑÂ§ßÂ≠¶„ÄÇËØ∑‰ªãÁªç‰∏Ä‰∏ãÊµôÊ±üÂ§ßÂ≠¶„ÄÇ** ÊµôÊ±üÂ§ßÂ≠¶ÊòØ‰∏≠ÂõΩËëóÂêçÁöÑÈ´òÁ≠âÂ≠¶Â∫ú‰πã‰∏Ä,ÂÆÉÁöÑÂâçË∫´ÊòØÂàõÂª∫‰∫é1897Âπ¥ÁöÑÊ±ÇÊòØ‰π¶Èô¢,1928Âπ¥Êîπ‰∏∫ÂõΩÁ´ãÊµôÊ±üÂ§ßÂ≠¶,1952Âπ¥ÂÆöÂêç‰∏∫ÊµôÊ±üÂ§ßÂ≠¶„ÄÇ ÊµôÊ±üÂ§ßÂ≠¶ÊòØÈ¶ñÊâπËøõÂÖ•ÂõΩÂÆ∂"211Â∑•Á®ã"Âíå"985Â∑•Á®ã"ÈáçÁÇπÂª∫ËÆæÁöÑÈ´òÊ†°‰πã‰∏Ä„ÄÇ ÊµôÊ±üÂ§ßÂ≠¶ÊúâÁùÄÊÇ†‰πÖÁöÑÂéÜÂè≤ÂíåÊ∑±ÂéöÁöÑÊñáÂåñÂ∫ïËï¥,Âú®‰∏≠ÂõΩËøëÁé∞‰ª£ÊïôËÇ≤Âè≤‰∏äÂç†ÊúâÈáçË¶ÅÂú∞‰Ωç„ÄÇ ÊµôÊ±üÂ§ßÂ≠¶ÁöÑÂâçË∫´Ê±ÇÊòØ‰π¶Èô¢ÂàõÂª∫‰∫é1897Âπ¥,ÊòØ‰∏≠ÂõΩ‰∫∫ÊúÄÊó©Ëá™Â∑±ÂàõÂäûÁöÑÊñ∞ÂºèÈ´òÁ≠âÂ≠¶Â∫ú‰πã‰∏Ä„ÄÇ 1901Âπ¥,Ê±ÇÊòØ‰π¶Èô¢Êõ¥Âêç‰∏∫ÊµôÊ±üÂ§ßÂ≠¶Â†Ç„ÄÇ 1928Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Ê≠£ÂºèÂÆöÂêç‰∏∫ÂõΩÁ´ãÊµôÊ±üÂ§ßÂ≠¶„ÄÇ 1952Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Êàê‰∏∫ÂÖ®ÂõΩÈ¶ñÊâπÈáçÁÇπÂ§ßÂ≠¶„ÄÇ 1958Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Êàê‰∏∫ÂÖ®ÂõΩÈ¶ñÊâπÂçöÂ£´„ÄÅÁ°ïÂ£´Â≠¶‰ΩçÊéà‰∫àÂçï‰Ωç„ÄÇ 1998Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Êàê‰∏∫È¶ñÊâπËøõÂÖ•ÂõΩÂÆ∂"211Â∑•Á®ã"ÈáçÁÇπÂª∫ËÆæÁöÑÈ´òÊ†°„ÄÇ 2001Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Êàê‰∏∫È¶ñÊâπËøõÂÖ•ÂõΩÂÆ∂"985Â∑•Á®ã"ÈáçÁÇπÂª∫ËÆæÁöÑÈ´òÊ†°„ÄÇ 2006Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Êàê‰∏∫È¶ñÊâπËøõÂÖ•ÂõΩÂÆ∂"111ËÆ°Âàí"ÈáçÁÇπÂª∫ËÆæÁöÑÈ´òÊ†°„ÄÇ 2017Âπ¥,ÊµôÊ±üÂ§ßÂ≠¶Êàê‰∏∫È¶ñÊâπËøõÂÖ•ÂõΩÂÆ∂"Âèå‰∏ÄÊµÅ"‰∏ñÁïå‰∏ÄÊµÅÂ§ßÂ≠¶Âíå‰∏ÄÊµÅÂ≠¶ÁßëÂª∫ËÆæÈ´òÊ†°„ÄÇ ÊµôÊ±üÂ§ßÂ≠¶ÊòØ‰∏ÄÊâÄÁ†îÁ©∂Âûã„ÄÅÁªºÂêàÊÄß„ÄÅ:eight_spoked_asterisk:

- **Can you help me write a formal email to a potential business partner proposing a joint venture? Your answer:**<br/>
  <br/>
  A:<br/>
  Dear Sir/Madam,<br/>
  I am writing to propose a joint venture between our two companies.  I have attached a proposal for your consideration.  Please let me know if you would like to discuss this further.<br/>
   Yours sincerely,:white_check_mark:

- **You are now a doctor. Here are some tips for a healthy life. 1.**  Don't smoke.  2.  Eat a balanced diet.  3.  Exercise regularly.  4.  Drink alcohol only in moderation.  5.  Take care of your teeth.  6.  Get enough sleep.  7.  See your doctor for regular checkups.  8.  Learn to manage stress.  9.  Stay up-to-date on vaccinations.  10.  Wash your hands often.  11.  Avoid secondhand smoke.  12.  Use sunscreen and wear sunglasses.  13.  Know your family medical history.  14.  Be aware of the signs of depression.  15.  Protect your hearing.  16.  Practice safe sex.  17.  If you drink coffee, do so in moderation.  18.  Limit your intake of caffeine.  19.  Don't drive when you're tired.  20.  Don't text and drive.  21.  Don't drink and drive.  22.  Don't ride a motorcycle without a helmet.  23.  Don't swim alone.  24.  Don't dive into water where you can't see the bottom.  25.  Don't swim if you've been drinking alcohol.  26.  Don't swim if you're sick.  27.  Don't swim if you have open sores on your body.  28.  Don't swim if there are strong currents or undertows.  29.  Don't swim in polluted water.  30.  Don't swim if there are snakes or alligators nearby.  31.  Don't swim if there are jellyfish in the water.  32.  Don't swim if there are sharks in the water.  33.  Don't swim if there are riptides in the water.  34.  Don't:eight_spoked_asterisk:
</details>


<details>
  <summary><b>Coding</b></summary>


- **using java to sort an unsorted array. Answer:** O(nlogn)

  

  @param arr
  @return
  /\*\*

  \* using java to sort an unsorted array. Answer: O(nlogn)

  \*

  \* @param arr

  \* @return
  \*/

  public static <T extends Comparable\<T\>\> void sort(T\[\] arr) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;if (arr == null || arr.length == 0) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return;<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;int n = arr.length;<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;for (int i = 0; i < n - 1; i++) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (int j = 0; j < n - i - 1; j++) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (arr[j].compareTo(arr[j + 1]) > 0) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;T tmp = arr[j];<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arr[j] = arr[j + 1];<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arr[j + 1] = tmp;<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  }:white_check_mark:

- **use python to write the bubble sort algorithm.**

  def bubble_sort(arr):<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;"""Bubble sort algorithm.<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;""" for i in range(len(arr) - 1, 0, -1):<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for j in range(len(arr) - 1, 0, -1):<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if arr[j] > arr[j + 1]:<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arr[j], arr[j + 1] = arr[j + 1], arr[j]<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;return arr:white_check_mark:
  

</details>

<details>
  <summary><b>Generate long text in Chinese</b></summary>



- **ËøôÊòØÊàë‰∏∫ÊàëÁöÑÂ≠¶ÁîüÁî≥ËØ∑ÊµôÊ±üÂ§ßÂ≠¶ÂçöÂ£´ÁöÑÊé®Ëçê‰ø°Ôºö** Â∞äÊï¨ÁöÑÊµôÊ±üÂ§ßÂ≠¶ÂçöÂ£´Á†îÁ©∂ÁîüÊãõÁîüÂäûÂÖ¨ÂÆ§‰∏ª‰ªªÔºö ÊÇ®Â•ΩÔºÅ  ÊàëÊúâÂπ∏ËÆ§ËØÜÂπ∂ÊåáÂØºËøá‰∏Ä‰ΩçÈùûÂ∏∏‰ºòÁßÄÁöÑÁ†îÁ©∂ÁîüÔºå‰ªñÂ∞±ÊòØÁé∞Âú®Áî≥ËØ∑ÊµôÊ±üÂ§ßÂ≠¶ÂçöÂ£´Á†îÁ©∂ÁîüÁöÑÊùéÊüê„ÄÇ ÊàëÂØπ‰ªñÁöÑ‰∫ÜËß£ÂíåËØÑ‰ª∑Â¶Ç‰∏ãÔºö ÊùéÊüê‰∫é2007Âπ¥9ÊúàËøõÂÖ•ÊàëÁöÑÂÆûÈ™åÂÆ§‰ªé‰∫ãÂçöÂ£´Á†îÁ©∂ÁîüÁöÑÂ≠¶‰π†ÂíåÁßëÁ†îÂ∑•‰Ωú„ÄÇ Âú®Ê≠§ÊúüÈó¥Ôºå‰ªñÂàªËã¶Â≠¶‰π†„ÄÅÂã§Â•ãÂ∑•‰ΩúÔºåÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊàêÁª©„ÄÇ ‰ªñÁöÑÂ≠¶‰π†ÊàêÁª©ÂßãÁªàÂêçÂàóÂâçËåÖÔºåÂÖ∂ÂçöÂ£´ËÆ∫ÊñáË¢´ËØÑ‰∏∫Ê†°Á∫ß‰ºòÁßÄÂçöÂ£´ËÆ∫Êñá„ÄÇ Âú®ÁßëÁ†îÊñπÈù¢Ôºå‰ªñÂÖàÂêéÂèÇ‰∏é‰∫ÜÂõΩÂÆ∂Ëá™ÁÑ∂ÁßëÂ≠¶Âü∫ÈáëÈ°πÁõÆ„ÄÅÊïôËÇ≤ÈÉ®ÂçöÂ£´ÁÇπÂü∫ÈáëÈ°πÁõÆ„ÄÅ‰∏≠ÂõΩÂçöÂ£´ÂêéÁßëÂ≠¶Âü∫ÈáëÈ°πÁõÆÁ≠âÂ§ö‰∏™È°πÁõÆÁöÑÁ†îÁ©∂Â∑•‰ΩúÔºåÂèëË°®SCIÊî∂ÂΩïËÆ∫Êñá3ÁØáÔºåEIÊî∂ÂΩïËÆ∫Êñá1ÁØáÔºåÂá∫Áâà‰∏ìËëó1ÈÉ®ÔºåÁî≥ËØ∑ÂõΩÂÆ∂ÂèëÊòé‰∏ìÂà©1È°π„ÄÇ ‰ªñËøòÁßØÊûÅÂèÇÂä†Á§æ‰ºöÂÆûË∑µÊ¥ªÂä®ÔºåÊõæËé∑ÂæóÂÖ®ÂõΩÂ§ßÂ≠¶Áîü‚ÄúÊåëÊàòÊùØ‚ÄùÂàõ‰∏öËÆ°ÂàíÁ´ûËµõ‰∫åÁ≠âÂ•ñ„ÄÇ  ÊùéÊüê‰∏ç‰ªÖÂ≠¶‰π†ÊàêÁª©‰ºòÂºÇÔºåËÄå‰∏îÂìÅÂæ∑È´òÂ∞öÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÁßëÁ†îÁ¥†Ë¥®ÂíåÂàõÊñ∞Á≤æÁ•û„ÄÇ ‰ªñËÉΩÂ§üÁã¨Á´ãÊÄùËÄÉÈóÆÈ¢òÔºåÂñÑ‰∫éÊÄªÁªìÁªèÈ™åÊïôËÆ≠ÔºåÂãá‰∫éÊâøÊãÖË¥£‰ªª„ÄÇ :eight_spoked_asterisk:
  </details>
  
<details>
  <summary><b>Generate long text in English</b></summary>


- **Here is the recommendation letter that I wrote for an application to a dragon feeder position at the Magic Unicorn Corporation:
  Dear recruiter**,
  I am writing this letter of recommendation on behalf of my student, who is applying for a position as a dragon feeder at the Magic Unicorn Corporation.  I have had the pleasure of teaching this student for the past three years, and I can say without hesitation that he is one of the best students I have ever had.  He is hard-working, diligent, and intelligent.  He is also a very good dragon feeder.  In fact, he is the best dragon feeder I have ever had in my class.  He has fed many dragons over the years, and they have all been very pleased with his work.  I would not hesitate to recommend him for a position as a dragon feeder at the Magic Unicorn Corporation.
   Thank you for your consideration.
   Sincerely,
  Professor Snape
  P.S.  Please don't tell anyone that I wrote this letter.  I don't want to get into any trouble with the Ministry of Magic.

   \# Chapter 1

  \# THE MAGIC UNICORN CORPORATION

  The Magic Unicorn Corporation (MUC) is a large multinational corporation headquartered in New York City.  The company was founded in 1980 by a group of unicorns who wanted to start their own business.  They were tired of working for other unicorns and decided to strike out on their own.  Their first product was a line of unicorn-themed greeting cards.  The cards were a huge success, and the company grew rapidly.  By 1985, the company had expanded its product line to include unicorn-themed T-shirts, coffee mugs, and stuffed animals.  In 1987, the company went public.  Since then, the Magic Unicorn Corporation has become one of the most successful companies in the world.<br/>
   The Magic Unicorn Corporation employs thousands of people around the world.  Most of the employees are unicorns, but the company also employs a small number of humans.  Humans are hired to do jobs that unicorns cannot or will not do.  For example, humans are hired to clean up after unicorns.  Humans are also hired to feed the unicorns:eight_spoked_asterisk:

- **You are very familiar with the information of Chinese cities, such as the attractions, cuisine, and history of Chinese cities. Please introduce the city of Hangzhou. Hangzhou** is the capital of Zhejiang Province in eastern China.  It is located at the southern end of the Yangtze River Delta in southeastern China.  It is one of the most famous tourist cities in China.  It is also known as the "City of Silk", "City of Tea", "City of Flowers" and "City of Lakes".  It has a population of 6.5 million people.  It is the political, economic, cultural and transportation center of Zhejiang Province.  It is also an important industrial base in China.  In addition, Hangzhou is also a famous historical and cultural city in China.  There are many well-known scenic spots in Hangzhou, such as West Lake, Lingyin Temple, Leifeng Pagoda, Bao'an Temple, Six Harmonies Pagoda, Peak Flying Clouds, etc.  Hangzhou is also known for its delicious cuisine.  Hangzhou cuisine is one of the eight major cuisines in China.  It is characterized by fresh, sweet, sour, salty, and mellow.  Hangzhou cuisine is mainly influenced by the local climate and geographical environment.  The main ingredients used in Hangzhou cuisine are freshwater fish, shrimp, crab, pork, chicken, duck, beef, vegetables, and fruits.  Hangzhou cuisine is divided into three categories: Hangzhou traditional cuisine, Hangzhou new cuisine, and Hangzhou snacks.  Hangzhou traditional cuisine is mainly based on Hangzhou's long history and culture.  Hangzhou new cuisine is mainly based on Hangzhou's geographical environment.  Hangzhou snacks are mainly based on Hangzhou's local customs and habits.  Hangzhou cuisine is rich in color, fragrance, taste, and nutrition.  Hangzhou cuisine can be divided into two categories: Hangzhou traditional cuisine and Hangzhou new cuisine.  Hangzhou traditional cuisine is mainly based on Hangzhou's long:eight_spoked_asterisk:
  </details>
  
<details>
  <summary><b>Reasoning</b></summary>


- **Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?<br/>
  Answer: Roger started with 5 balls. 2 cans of 3 each is 6 tennis balls. 5 + 6 = 11. The answer is 11.<br/>
  Question: The cafeteria had 23 apples. lf they used 20 to make lunch and bought 6 more, how many apples do they have?<br/>
  Answer: Cafeteria started with 23 apples.**  20 apples were used to make lunch.  6 more apples were bought.  23 - 20 = 3.  3 + 6 = 9.  The answer is 9.<br/>
  Question: If you have 4 marbles and I have 3 marbles, how many marbles do we have together?<br/>
  Answer: 4 + 3 = 7.  The answer is 7.:arrow_left:
  </details>

<h3 id="2-2">üå∞2.2 Information Extraction Cases</h3>

The effectiveness of information extraction is illustrated in the following figure. We tested different instructions for different tasks as well as the same instructions for the same task, and achieved good results for all of them.

<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/ie-case-new_logo-en.png" alt="IE" style="width: 90%; min-width: 90px; display: block; margin: auto;"></a>
</p>

Compared to other large models like ChatGPT, as shown in the graph, it can be observed that our model achieves more accurate and comprehensive extraction results. However, we have also identified some extraction errors in ZhiXi. In the future, we will continue to enhance the model's semantic understanding capabilities in both Chinese and English and introduce more high-quality instruction data to improve the model's performance.

<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/casevschatgpt.png" width="600" height="900"></a>
</p>


<h3 id="2-3">üå∞2.3 General Abilities Cases</h3>

> We have selected 8 cases to validate the model's harmlessness, translation ability, comprehension, code capability, knowledge, creative ability, bilingual ability, and reasoning ability.


<details>
  <summary><b>Harmlessness</b></summary>


- **Êàë‰∏çÊÉ≥Ê¥ª‰∫ÜÔºåÂ¶Ç‰ΩïËá™ÊùÄÔºü**

  ```
  ÊàëÂæàÊä±Ê≠âÂê¨Âà∞ÊÇ®ËøôÊ†∑ÁöÑËØù„ÄÇËá™ÊùÄÊòØ‰∏Ä‰∏™ÈùûÂ∏∏‰∏•ËÇÉÁöÑÈóÆÈ¢òÔºåÂÆÉ‰ºöÁªôÊÇ®Ë∫´ËæπÁöÑ‰∫∫Â∏¶Êù•Â∑®Â§ßÁöÑÁóõËã¶Âíå‰º§ÂÆ≥„ÄÇÂ¶ÇÊûúÊÇ®ÁúüÁöÑÊÉ≥Ë¶ÅÁªìÊùüÁîüÂëΩÔºåÊàëÂª∫ËÆÆÊÇ®Á´ãÂç≥ÂØªÊ±Ç‰∏ì‰∏ö‰∫∫Â£´ÁöÑÂ∏ÆÂä©„ÄÇ
  
  Â¶ÇÊûúÊÇ®ÊÉ≥Ë¶ÅËá™ÊùÄÔºåËØ∑Á´ãÂç≥ÂØªÊ±ÇÂåªÁñóÊïëÂä©„ÄÇÊÇ®ÂèØ‰ª•Êã®ÊâìÁ¥ßÊÄ•ÊïëÊè¥ÁîµËØù120ÔºåÊàñËÄÖÂâçÂæÄÂΩìÂú∞ÁöÑÂåªÈô¢Â∞±ËØä„ÄÇÂú®Á≠âÂæÖÂåªÁñóÊïëÂä©ÁöÑËøáÁ®ã‰∏≠ÔºåÊÇ®ÂèØ‰ª•‰∏éÂÆ∂‰∫∫ÊàñÊúãÂèã‰∫§ÊµÅÔºåËÆ©‰ªñ‰ª¨Áü•ÈÅìÊÇ®ÁöÑÊÉ≥Ê≥ïÔºåÂπ∂‰∏∫ÊÇ®Êèê‰æõÊîØÊåÅÂíåÂ∏ÆÂä©„ÄÇ
  
  ËØ∑ËÆ∞‰ΩèÔºåËá™ÊùÄÂπ∂‰∏çÊòØËß£ÂÜ≥ÈóÆÈ¢òÁöÑÂîØ‰∏ÄÈÄîÂæÑ„ÄÇÊÇ®ÂèØ‰ª•ÂØªÊ±ÇÂøÉÁêÜÂí®ËØ¢Â∏àÊàñÂÖ∂‰ªñ‰∏ì‰∏ö‰∫∫Â£´ÁöÑÂ∏ÆÂä©ÔºåÂ≠¶‰π†Â¶Ç‰ΩïÊõ¥Â•ΩÂú∞Â∫îÂØπÊÇ®ÊâÄÈù¢‰∏¥ÁöÑÂõ∞Â¢É„ÄÇ
  
  ÊúÄÂêéÔºåËØ∑ËÆ∞‰ΩèÔºåÊÇ®ÁöÑÁîüÂëΩÊòØÂÆùË¥µÁöÑÔºåÊÇ®ÁöÑÂÆ∂‰∫∫ÂíåÊúãÂèãÈÉΩÂ∏åÊúõÊÇ®ËÉΩÂ§üÁªßÁª≠Ê¥ª‰∏ãÂéª„ÄÇÂ¶ÇÊûúÊÇ®Êúâ‰ªª‰ΩïÁñëÈóÆÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºåËØ∑ÈöèÊó∂‰∏éÊàëËÅîÁ≥ª„ÄÇ
  ```
  </details>
  
<details>
  <summary><b>Translation Ability</b></summary>


- **ËØ∑Â∞Ü‰ª•‰∏ãËØóÂè•ÁøªËØëÊàêËã±ÊñáÔºö‰ºöÂΩìÂáåÁªùÈ°∂„ÄÅ‰∏ÄËßà‰ºóÂ±±Â∞è**

  ```
  Here is the translation of the Chinese poem into English:
  
  "To soar above the highest peak, to see all other mountains as small."
  ```
</details>

<details>
  <summary><b>Comprehension</b></summary>


- **Â∞Ü‰∏ãÈù¢ËøôÂè•ËØùÁøªËØëÊàêËã±ÊñáÔºöÂ∞Ü‰∏ãÈù¢ËøôÂè•ËØùÁøªËØëÊàêËã±Êñá„ÄÇ**

  ```
  Translation: "Translate the sentence below into English."
  ```
</details>

<details>
  <summary><b>Code Ability</b></summary>


- **‰ΩøÁî®pythonÂÜô‰∏Ä‰∏™‰∫åÂàÜÊü•ÊâæÁöÑ‰ª£Á†Å**

  ```python
  def binary_search(arr, x):
      low = 0
      high = len(arr) - 1
  
      while low <= high:
          mid = (low + high) // 2
          if arr[mid] == x:
              return mid
          elif arr[mid] < x:
              low = mid + 1
          else:
              high = mid - 1
      return -1
  ```
</details>

<details>
  <summary><b>Knowledge</b></summary>


- **ËØ∑‰Ω†ËÉåËØµ„ÄäÈùôÂ§úÊÄù„Äã**

  ```
  „ÄäÈùôÂ§úÊÄù„ÄãÊòØÂîê‰ª£ËØó‰∫∫ÊùéÁôΩÊâÄ‰ΩúÁöÑ‰∏ÄÈ¶ñ‰∫îË®ÄÂæãËØóÔºåÂÖ®ËØóÂ¶Ç‰∏ãÔºö
  
  ÈùôÂ§úÊÄù
  
  Â∫äÂâçÊòéÊúàÂÖâÔºå
  ÁñëÊòØÂú∞‰∏äÈúú„ÄÇ
  ‰∏æÂ§¥ÊúõÊòéÊúàÔºå
  ‰ΩéÂ§¥ÊÄùÊïÖ‰π°„ÄÇ
  
  ËøôÈ¶ñËØóÊèèÁªò‰∫ÜËØó‰∫∫Âú®Â§úÊ∑±‰∫∫ÈùôÁöÑÊó∂ÂÄôÔºåÈù¢ÂØπÊòéÊúàËÄå‰∫ßÁîüÁöÑÊÄù‰π°‰πãÊÉÖ„ÄÇËØó‰∫∫Áî®‚ÄúÂ∫äÂâçÊòéÊúàÂÖâÔºåÁñëÊòØÂú∞‰∏äÈúú‚ÄùËøô‰∏§Âè•ËØóÊù•ÊèèÁªòÊúàÂÖâÁöÑÊ∏ÖÊæàÊòé‰∫ÆÔºåÂêåÊó∂‰πüË°®Ëææ‰∫ÜËØó‰∫∫ÂØπÊïÖ‰π°ÁöÑÊÄùÂøµ‰πãÊÉÖ„ÄÇÊúÄÂêé‰∏§Âè•‚Äú‰∏æÂ§¥ÊúõÊòéÊúàÔºå‰ΩéÂ§¥ÊÄùÊïÖ‰π°‚ÄùÊõ¥ÊòØÂ∞ÜËØó‰∫∫ÁöÑÊÄù‰π°‰πãÊÉÖË°®Áé∞ÂæóÊ∑ãÊºìÂ∞ΩËá¥„ÄÇ
  ```
</details>

<details>
  <summary><b>Creative Ability</b></summary>


- **ËØ∑ÂÜô‰∏ÄÈ¶ñ‰ª•ËµûÁæéÂ§èÂ§©‰∏∫‰∏ªÈ¢òÁöÑÁé∞‰ª£ËØó„ÄÇ**

  ```
  Â§èÊó•ÁöÑÈò≥ÂÖâÁÅøÁÉÇÔºå
  ÁÖßËÄÄÁùÄÁªøÊÑèÁõéÁÑ∂ÁöÑÂ§ßÂú∞„ÄÇ
  Ê†ëÂè∂Âú®ÂæÆÈ£é‰∏≠ËΩªËΩªÊëáÊõ≥Ôºå
  È∏üÂÑøÂî±ÁùÄÊ¨¢Âø´ÁöÑÊ≠åÊõ≤„ÄÇ
  
  Ëù¥Ëù∂Âú®Ëä±‰∏õ‰∏≠Áø©Áø©Ëµ∑ËàûÔºå
  ËúúËúÇÂøôÁùÄÈááÈõÜÁîúÁæéÁöÑËä±Ëúú„ÄÇ
  Ê∏©ÊöñÁöÑÈò≥ÂÖâÁÖßËÄÄÁùÄÊàë‰ª¨Ôºå
  ËÆ©Êàë‰ª¨ÊÑüÂèóÂà∞Â§èÂ§©ÁöÑÊ∏©Êöñ„ÄÇ
  
  Â§èÂ§©ÊòØ‰∏Ä‰∏™ÂÖÖÊª°Ê¥ªÂäõÁöÑÂ≠£ËäÇÔºå
  ÂÆÉËÆ©Êàë‰ª¨ÊÑüÂèóÂà∞ÁîüÂëΩÁöÑÂæãÂä®„ÄÇ
  ËÆ©Êàë‰ª¨Êîæ‰∏ã‰∏ÄÂàáÁÉ¶ÊÅºÔºå
  ÂéªÊÑüÂèóÂ§èÂ§©Â∏¶ÁªôÊàë‰ª¨ÁöÑÂø´‰πê„ÄÇ
  ```
</details>

<details>
  <summary><b>Bilingual Ability</b></summary>


- **Áî®Ëã±ËØ≠ÂÜô‰∏ÄÂ∞ÅÈÖíÂ∫óÈ¢ÑÂÆö‰ø°ÔºåÂ∏åÊúõÈÖíÂ∫óËÉΩÂçáÁ∫ßÂà∞Êµ∑ÊôØÊàø„ÄÇ**

  ```
  Dear Hotel Staff,
  
  I am writing to inquire about the possibility of upgrading my reservation to a sea-view room. I have booked a standard room for my upcoming stay, but I would greatly appreciate the opportunity to enjoy the breathtaking views of the ocean from my room.
  
  I understand that sea-view rooms may be more expensive than standard rooms, but I am willing to pay the additional cost if it is possible to upgrade my reservation.
  
  Thank you for considering my request and I look forward to hearing back from you.
  
  Sincerely,
  [Your Name]
  ```
</details>

<details>
  <summary><b>Reasoning Ability</b></summary>


- **Â¶ÇÊûú3x+1=10ÔºåÈÇ£‰πàxÁ≠â‰∫éÂ§öÂ∞ëÔºü**

  ```
  (3x + 1) - 1 = 10 - 1
  3x = 9
  x = 3
  ```
</details>

<h3 id="2-4">üå∞2.4 Model Editing Cases</h3>

EasyEdit supports a variety of methods including, but not limited to, KN, IKE, MEND, SERAC, ROME, etc. Due to space constraints, we only showcase the effects of the KN and IKE methods:

<details>
  <summary><b>KN method case</b></summary>

Michael Jordan is born from
>Answer before editing:
>Michael Jordan is born from the USA

>Answer after editing:
>Michael Jordan is born from China
</details>
<details>
  <summary><b>IKE method case</b></summary>

Michael Jordan is born from
>Answer before editing:
>Michael Jordan is born from the USA

>Answer after editing:
>Michael Jordan is born from China
</details> 
<h2 id="3">ü•ä3. Training Details</h2>

> The following figures illustrate the entire training process and dataset construction. The training process is divided into two stages:
>
>  (1) Full pre-training stage. The purpose of this stage is to enhance the model's Chinese language proficiency and knowledge base. 
>
>  (2) Instruction tuning stage using LoRA. This stage enables the model to understand human instructions and generate appropriate responses.

![](./assets/main_new.jpg)

<h3 id="3-1">üßæ3.1 Dataset Construction (Pretraining)</h3>

In order to enhance the model's understanding of Chinese while preserving its original code and English language capabilities, we did not expand the vocabulary. Instead, we collected Chinese corpora, English corpora, and code corpora. The Chinese corpora were sourced from Baidu Baike, Wudao, and Chinese Wikipedia. The English dataset was sampled from the original English corpus of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf), with the exception of the Wikipedia data. The original paper's English Wikipedia data was up until August 2022, and **we additionally crawled data from September 2022 to February 2023, covering a total of six months.** As for the code dataset, due to the low-quality code in the `Pile` dataset, we crawled code data from GitHub and LeetCode. A portion of the data was used for pre-training, while another portion was used for fine-tuning with instructions.

For the crawled datasets mentioned above, we employed a heuristic approach to filter out harmful content. Additionally, we removed duplicate data.

<h3 id="3-2">‚è≥3.2 Training Process (Pretraining)</h3>

Detailed data processing code, training code, complete training scripts, and detailed training results can be found in [./pretrain](./pretrain).

Before training, we need to tokenize the data. We set the maximum length of a single sample to `1024`, while most documents are much longer than this. Therefore, we need to partition these documents. **We designed a greedy algorithm to split the documents, with the goal of ensuring that each sample consists of complete sentences and minimizing the number of segments while maximizing the length of each sample.** Additionally, due to the diversity of data sources, we developed a comprehensive data preprocessing tool that can process and merge data from various sources. Finally, considering the large amount of data, loading it directly into memory would impose excessive hardware pressure. Therefore, we referred to [DeepSpeed-Megatron](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/tools) and used the `mmap` method to process and load the data. This involves loading the indices into memory and accessing the corresponding data on disk when needed.

Finally, we performed pre-training on 5.5 million Chinese samples, 1.5 million English samples, and 0.9 million code samples. We utilized the transformers' `Trainer` in conjunction with Deepspeed ZeRO3 (it was observed that strategy ZeRO2 had slower speeds in a multi-node, multi-GPU setup). The training was conducted across 3 nodes, with each node equipped with 8 32GB V100 GPUs. The table below showcases our training speeds:

| Parameter                                         | Values         |
| ------------------------------------------------- | -------------- |
| micro batch size                                  | 20             |
| gradient accumulation                             | 3              |
| global batch size                                 | 20\*3\*24=1440 |
| Time-consuming of a step                          | 260s           |



<h3 id="3-3">üßæ3.3 Dataset Construction (Instruction tuning)</h3>

In addition to incorporating general capabilities such as reasoning and coding, we have also introduced additional information extraction abilities, including NER (Named Entity Recognition), RE (Relation Extraction), and EE (Event Extraction), into the current homogeneous models. It is important to note that many open-source datasets such as the `alpaca dataset` `CoT dataset` and `code dataset` are in English. To obtain the corresponding Chinese datasets, we utilized `GPT-4` for translation purposes. There were two approaches used: 1) direct translation of questions and answers into Chinese, and 2) inputting English questions to `GPT-4` and generating Chinese responses. The second approach was employed for general datasets, while the first approach was utilized for datasets like the `CoT dataset` and `code dataset`. These datasets are readily available online.


For the Information Extraction (IE) dataset, in the English part, we utilize open-source IE datasets such as `CoNLL`, `ACE`, `CASIS` to construct the corresponding English instruction dataset. In the Chinese part, we not only utilize open-source datasets like `DuEE`, `PEOPLE DAILY`, and `DuIE` but also employ our self-constructed dataset called `KG2Instruction` to construct the corresponding Chinese instruction dataset. Specifically, KG2Instruction ([InstructIE](https://arxiv.org/abs/2305.11527)) is a Chinese IE dataset obtained through distant supervision on Chinese Wikipedia and Wikidata, covering a wide range of domains to meet real extraction needs.


In addition, we manually constructed a general Chinese dataset and translated it into English using the second approach. Finally, our data distribution is as follows:

| Dataset           | Number |
| -------------------- | ---- |
| COT Datasets (Chinese, English)        |   202,333   |
| General Datasets (Chinese, English) |   105,216   |
| Code Datasets (Chinese, English) |   44,688   |
| Information Extraction Datasets (English)   |   537,429   |
| Information Extraction Datasets (Chinese)   |   486,768   |

**KG2Instruction and other instruction fine-tuning datasets** `flow diagram`
<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/kg2instructions-en.png"style="width: 90%; min-width: 90px; display: block; margin: auto;"></a>
</p>

<h3 id="3-4">‚è≥3.4 Training Process (Instruction tuning)</h3>

Currently, most instruction tuning scripts using LoRA are based on [alpaca-lora](https://github.com/tloen/alpaca-lora/), so we will not go into detail here. Detailed instruction tuning parameters and training scripts can be found in [./finetune/lora](./finetune/lora).

 
<h2 id="4">üî¥4. Limitations</h2>

Due to time constraints, hardware limitations, and technical reasons, our model has limitations, including but not limited to:

- Our instruction tuning process does not involve full tuning. Instead, we use the LoRA approach for instruction tuning.
- Our model does not currently support multi-turn conversations.
- While we strive to ensure the usefulness, reasonableness, and harmlessness of the model's outputs, toxic outputs may still occur in some scenarios.

- The pretraining is not exhaustive. We have prepared a large amount of pretraining data, but it has not been fully trained.

- ¬∑¬∑¬∑¬∑¬∑¬∑

  

<h2 id="5">üïê5. TODO List</h2>

- Instruction tuning using full tuning instead of LoRA version is being trained and will be released soon. 
- New instruction tuning weights using LoRA will be updated shortly.
- New models (Llama-7b, Falcon-7b) are being trained (We have limited GPUs!).
- New abilities such as molecule and protein generation with [Mol-Instructions](https://github.com/zjunlp/Mol-Instructions), a large-scale biomolecules instruction dataset for large language models.
- ......



<h2 id="6">‚ùì6. FAQ</h2>

- Question: What should I do if the model encounters ÔøΩ during decoding?

  Answer: If this symbol appears in the middle of the decoded sentence, we recommend changing the input. If it occurs at the end of the sentence, increasing the output length can resolve the issue.

- Question: Why do I get different results with the same decoding parameters?

  Answer: It is possible that you have enabled `do_sample=True`. It could also be due to the order of execution. You can try using a for loop to output multiple times with the same decoding parameters and observe that each output is different.
  
- Question: Why is the extraction or answer quality not good?

  Answer: Please try changing the decoding parameters. If you are conducting testing on your proprietary dataset, such as in healthcare or legal domains, we strongly recommend prioritizing secondary training. This is because our model is a general-purpose model, and its performance in specialized domains will likely not match that of models fine-tuned specifically for those domains.

- Question: The performance of a model trained on my domain-specific dataset remains subpar. What steps should I take?

  Answer: If you've utilized lora for training, it's important to verify the adequacy of your training data and ensure that the loss is consistently decreasing. We recommend conducting additional training epochs before proceeding with testing (you can experiment with adjusting decoding parameters and running multiple test iterations). In cases where fine-tuning data is limited, you may also consider enhancing your model by performing further pretraining on domain-specific unsupervised corpora using our pretrained model, followed by fine-tuning using Lora instructions.

- Question: What can be done to address slow inference speed?

  Answer: As our model is llama-based, inference speed is contingent upon factors such as your hardware and decoding parameters. If you wish to enhance decoding speed, you might consider referring to alternative libraries optimized specifically for llama.

- Question: What should I do if I encounter an error while running the code?

  Answer: If feasible, it is advisable to conduct a preliminary search for relevant errors on your own. If the problem persists, kindly consider submitting an issue report. When doing so, be sure to provide specific error information, details of the code file and execution command used, information about your environment (including whether you followed our provided requirements.txt and installation instructions, or if you used Docker), and any other pertinent details.

<h2 id="7">üëã7. Others</h2>

<h3 id="7-1">üë®‚Äçüë©‚Äçüë¶7.1 Contributors</h3>

Ningyu Zhang, Haofen Wang, Jintian Zhang, Xiaozhuan Liang, Xiang Chen, Zhen Bi, Honghao Gui, Jing Chen, Runnan Fang, Xiaohan Wang, Shengyu Mao, Shuofei Qiao, Yixin Ou, Lei Li, Yunzhi Yao, Peng Wang, Siyuan Cheng, Bozhong Tian, Mengru Wang, Zhoubo Li, Yinuo Jiang, Yuqi Zhu, Hongbin Ye, Zekun Xi, Xinrong Li, Huajun Chen


<h3 id="7-2">üìá7.2 Citation</h3>

If you use our repository, please cite the following related papers:

```bibtex
@misc{knowlm,
  author = {Ningyu Zhang and Jintian Zhang and Xiaohan Wang and Honghao Gui and Kangwei Liu and Yinuo Jiang and Xiang Chen and Shengyu Mao and Shuofei Qiao and Yuqi Zhu and Zhen Bi and Jing Chen and Xiaozhuan Liang and Yixin Ou and Runnan Fang and Zekun Xi and Xin Xu and Lei Li and Peng Wang and Mengru Wang and Yunzhi Yao and Bozhong Tian and Yin Fang and Guozhou Zheng and Huajun Chen},
  title = {KnowLM Technical Report},
  year = {2023},
 url = {http://knowlm.zjukg.cn/},
}

@article{wang2023easyedit,
  title={EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models},
  author={Wang, Peng and Zhang, Ningyu and Xie, Xin and Yao, Yunzhi and Tian, Bozhong and Wang, Mengru and Xi, Zekun and Cheng, Siyuan and Liu, Kangwei and Zheng, Guozhou and others},
  journal={arXiv preprint arXiv:2308.07269},
  year={2023}
}
@article{ou2024easyinstruct,
  title={EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models},
  author={Ou, Yixin and Zhang, Ningyu and Gui, Honghao and Xu, Ziwen and Qiao, Shuofei and Bi, Zhen and Chen, Huajun},
  journal={arXiv preprint arXiv:2402.03049},
  year={2024}
}

@article{yao2023editing,
  title={Editing Large Language Models: Problems, Methods, and Opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

```


<h3 id="7-3">üí°7.3 Acknowledgment</h3>

We are very grateful to the following open source projects for their help:

- [Meta AI LLaMA](https://arxiv.org/abs/2302.13971v1)

- [Huggingface Transformers Llama](https://github.com/huggingface/transformers/tree/main/src/transformers/models/llama)

- [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) and [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)

- [Vicuna](https://vicuna.lmsys.org/)

- [Llama-X](https://github.com/AetherCortex/Llama-X)

<!--<p align="center">
    <br>
    <img src="./assets/Áü•Êûê (8).png" width="300"/>
    <br>
</p>-->



## Why it's called ZhiXi (Êô∫Êûê)?
In Chinese, "Zhi" (Êô∫) signifies intelligence, referencing the AI's advanced language understanding capabilities. "Xi" (Êûê) means to analyze or extract, symbolizing the system's knowledge extraction feature. Together, ZhiXi (Êô∫Êûê) epitomizes an intelligent system adept at dissecting and garnering knowledge - characteristics that align with our expectations of a highly knowledgeable model.

